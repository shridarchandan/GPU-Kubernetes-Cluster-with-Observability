apiVersion: v1
kind: Pod
metadata:
  name: pytorch-inference
spec:
  restartPolicy: Never
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  containers:
  - name: pytorch-gpu
    image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
    command: ["python", "-c"]
    args:
    - |
      import torch
      import torchvision.models as models
      import time
      
      print("GPU Available:", torch.cuda.is_available())
      print("GPU Device:", torch.cuda.get_device_name(0))
      
      # Load pre-trained ResNet50 model
      model = models.resnet50(pretrained=True).cuda()
      model.eval()
      
      # Simulate inference
      batch_size = 16
      input_data = torch.randn
