apiVersion: v1
kind: Pod
metadata:
  name: pytorch-inference-long
spec:
  restartPolicy: Never
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  containers:
  - name: pytorch-gpu
    image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
    command: ["python", "-c"]
    args:
    - |
      import torch
      import torchvision.models as models
      import time
      
      print("GPU Available:", torch.cuda.is_available())
      print("GPU Device:", torch.cuda.get_device_name(0))
      
      model = models.resnet50(pretrained=True).cuda()
      model.eval()
      
      batch_size = 32
      input_data = torch.randn(batch_size, 3, 224, 224).cuda()
      
      print("Running inference on GPU for 5 minutes...")
      for i in range(300):
        with torch.no_grad():
          output = model(input_data)
        if i % 10 == 0:
          print(f"Batch {i}/300 complete")
        time.sleep(1)
      
      print("Complete!")
    resources:
      limits:
        nvidia.com/gpu: 1
